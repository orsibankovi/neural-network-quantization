{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6NkvX1BYpKey"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import os\n",
        "import torch.optim as optim\n",
        "import torchmetrics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import natsort\n",
        "import numpy as np\n",
        "from torch.quantization import quantize_fx\n",
        "from torch.ao.quantization import QConfigMapping\n",
        "import copy\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN7i5FGApOMk",
        "outputId": "35cb2418-7eeb-4342-9854-12ba4b8746e6"
      },
      "outputs": [],
      "source": [
        "dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vIv24K0ZpKe0"
      },
      "outputs": [],
      "source": [
        "preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "class GetDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        super(GetDataset, self).__init__()\n",
        "        self.path_input = \"C:/Users/orsolya.bankovi/Documents/Uni/deepLearning_project/D41_ILRSVRC2012_224\"\n",
        "        self.input = os.listdir(self.path_input)\n",
        "        self.input_names = list(filter(lambda x: x.endswith(\".png\"), list(self.input)))\n",
        "        self.sorted_input = natsort.natsorted(self.input_names)\n",
        "        self.input_images = []\n",
        "        for count, images in enumerate(self.sorted_input[:1000]):\n",
        "            input_image = Image.open(self.path_input + '/' + images)\n",
        "            input_tensor = preprocess(input_image).float()\n",
        "            self.input_images.append(input_tensor)\n",
        "            \n",
        "        self.labels = []\n",
        "        with open(\"C:/Users/orsolya.bankovi/Documents/Uni/deepLearning_project/D41_ILRSVRC2012_224/labels.txt\", 'r') as f:\n",
        "            for i in range(1000):\n",
        "                line = f.readline()\n",
        "                self.labels.append(int(line))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.input_images[index], self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NZAfp9kIpKe1"
      },
      "outputs": [],
      "source": [
        "class ImageNetTest():\n",
        "    def __init__(self, batch_size):\n",
        "        super(ImageNetTest, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "    def test(self, net, testset, dev):\n",
        "        test_loader = torch.utils.data.DataLoader(testset, self.batch_size, shuffle=False)\n",
        "        net = net.to(dev)\n",
        "        net.train(False)\n",
        "        count = 0\n",
        "        correct = 0\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data = data.to(dev)\n",
        "                target = target.to(dev)\n",
        "                output = net(data)\n",
        "                probabilities = torch.nn.functional.softmax(output, dim=1)\n",
        "                pred = probabilities.argmax(dim=1, keepdim=True)\n",
        "                if pred == target:\n",
        "                    correct += 1\n",
        "                count += 1\n",
        "        print('Accuracy: ', round(correct/count, 4))\n",
        "        return round(correct/count, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Z7nbhMYEpKe3"
      },
      "outputs": [],
      "source": [
        "def quantize_model(model, calibdata, dev, backend=\"fbgemm\"):\n",
        "    torch.no_grad()\n",
        "    m = copy.deepcopy(model).to(dev)\n",
        "    m.eval()\n",
        "\n",
        "    example_inputs = torch.unsqueeze(calibdata.input_images[10], dim=0).to(dev)\n",
        "    qconfig = torch.quantization.get_default_qconfig(backend)\n",
        "    qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
        "\n",
        "    model_prepared = quantize_fx.prepare_fx(m, qconfig_mapping, example_inputs)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for i in range(10):\n",
        "            x = torch.unsqueeze(calibdata.input_images[i], dim=0).to(dev)\n",
        "            model_prepared(x)\n",
        "            \n",
        "    model_quantized = quantize_fx.convert_fx(model_prepared)\n",
        "\n",
        "    return model_quantized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RFR7gd98uEUN"
      },
      "outputs": [],
      "source": [
        "class PTQ():\n",
        "    def __init__(self, model, calibdata, dev):\n",
        "        super(PTQ, self).__init__()\n",
        "        self.model = model\n",
        "        self.calibdata = calibdata\n",
        "        self.dev = dev\n",
        "        self.hooks_names = []\n",
        "        self.hook_values = []\n",
        "        self.quant_hook_values = []\n",
        "\n",
        "        self.target_modules = {'Conv2d', 'Linear', 'ConvTranspose2d', 'Conv1d', 'Conv3d'}\n",
        "        \n",
        "        self.indices = [100, 214, 220, 544, 831]\n",
        "\n",
        "    def get_hooks(self, model, target_modules, quantized=False):\n",
        "        \"\"\"Get hooks for given model and target modules.\"\"\"\n",
        "        if quantized:\n",
        "            self.quant_hook_values.clear()\n",
        "        else:    \n",
        "            self.hook_values.clear()\n",
        "        m = copy.deepcopy(model)\n",
        "\n",
        "        def hook_fn(module, input, output):\n",
        "            if quantized:\n",
        "                self.quant_hook_values.append(torch.dequantize(output.detach().cpu()))\n",
        "            else:\n",
        "                self.hook_values.append(output.detach().cpu().float())\n",
        "\n",
        "        self.hooks_names.clear()\n",
        "        hooks = []\n",
        "        for module in m.named_modules():\n",
        "            if module[1].__class__.__name__ in target_modules:\n",
        "                self.hooks_names.append(module[0])\n",
        "                hook = module[1].register_forward_hook(hook_fn)\n",
        "                hooks.append(hook)\n",
        "                \n",
        "        rand_inputs = [self.calibdata.input_images[i] for i in self.indices]\n",
        "        with torch.inference_mode():\n",
        "            x = torch.stack(rand_inputs, dim=0).to(self.dev)\n",
        "            m(x)\n",
        "\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "\n",
        "    def get_module(self, model, name):\n",
        "        return dict(model.named_modules())[name]\n",
        "\n",
        "    def get_param(self, module, attr):\n",
        "        param = getattr(module, attr, None)\n",
        "        if callable(param):\n",
        "            return param()\n",
        "        else:\n",
        "            return param\n",
        "\n",
        "    def get_quantized_model(self, model, backend):\n",
        "        torch.no_grad()\n",
        "        m = copy.deepcopy(model)\n",
        "        m.eval()\n",
        "\n",
        "        qconfig = torch.quantization.get_default_qconfig(backend)\n",
        "        qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
        "\n",
        "        m = quantize_fx.fuse_fx(m)\n",
        "        m = torch.quantization.QuantWrapper(m)\n",
        "        model_prepared = torch.quantization.prepare(m, qconfig_mapping)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            for i in range(10):\n",
        "                x = torch.unsqueeze(self.calibdata.input_images[i], dim=0).to(self.dev)\n",
        "                model_prepared(x)\n",
        "\n",
        "        model_quantized = torch.quantization.convert(model_prepared)\n",
        "\n",
        "        self.get_hooks(model_quantized, target_modules=self.target_modules,\n",
        "                       quantized=True)\n",
        "\n",
        "        return model_quantized\n",
        "\n",
        "\n",
        "    def bias_correction(self, model, target_modules, backend):\n",
        "        i=0\n",
        "        for _, submodule in model.named_modules():\n",
        "            if submodule.__class__.__name__ in target_modules:\n",
        "                bias = self.get_param(submodule, 'bias')\n",
        "                if bias is not None:\n",
        "                    self.get_quantized_model(model, backend)\n",
        "                    f = self.quant_hook_values[i]\n",
        "                    g = self.hook_values[i]\n",
        "                    eps = torch.mean(f - g)\n",
        "\n",
        "                    #print(\"eps data:\", eps)\n",
        "                    bias.data[::] -= eps\n",
        "                i += 1\n",
        "        return model\n",
        "\n",
        "    def quantize_model(self, backend=\"fbgemm\"):\n",
        "        torch.no_grad()\n",
        "        m = copy.deepcopy(self.model).to(self.dev)\n",
        "        m.eval()\n",
        "\n",
        "        self.get_hooks(self.model, target_modules=self.target_modules)\n",
        "\n",
        "        model_bias_corr = self.bias_correction(m, target_modules=self.target_modules, backend=backend)\n",
        "        model_bias_corr.eval()\n",
        "\n",
        "        model_quantized_bias_corr = quantize_model(model_bias_corr, self.calibdata, self.dev, backend)\n",
        "\n",
        "        return model_quantized_bias_corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QuantizationAwareTraining():\n",
        "    def __init__(self, trainset, testset, batch_size):\n",
        "        super(QuantizationAwareTraining, self).__init__()\n",
        "        self.dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.batch_size = batch_size\n",
        "        self.trainset = trainset\n",
        "        self.testset = testset\n",
        "        self.Dice = torchmetrics.Dice(zero_division=1.0, threshold=0.5).to(self.dev)\n",
        "\n",
        "    def train(self, net):\n",
        "        train_loader = torch.utils.data.DataLoader(self.trainset, self.batch_size, shuffle=True)\n",
        "        criterion = nn.BCELoss().to(self.dev)\n",
        "        optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
        "        net.train(True)\n",
        "        best_epoch = 0\n",
        "        best_dice = 0.0\n",
        "        best_net = net\n",
        "\n",
        "        for epoch in range(10):            \n",
        "            dice_losses = []\n",
        "        \n",
        "            for data, target in train_loader:\n",
        "                data = data.to(self.dev)\n",
        "                target = target.to(self.dev)\n",
        "                optimizer.zero_grad()  # clear the gradient\n",
        "\n",
        "                output = net(data)  # forward propagation\n",
        "                loss = criterion(output, target)  # calculate loss\n",
        "                \n",
        "                loss.backward()  # current loss\n",
        "                optimizer.step()  # update parameters\n",
        "\n",
        "                dice_loss_value = self.Dice(output, target.int())\n",
        "                dice_losses.append(dice_loss_value.item())\n",
        "            \n",
        "            if best_dice < np.average(dice_losses[-len(train_loader)//self.batch_size:]):\n",
        "                best_dice = np.average(dice_losses[-len(train_loader)//self.batch_size:])\n",
        "                best_epoch = epoch\n",
        "                best_net = net\n",
        "        print('best epoch: ', best_epoch)\n",
        "        print('best dice: ', best_dice)            \n",
        "        \n",
        "        return best_net\n",
        "\n",
        "    def quantize_model(self, model, calibdata):\n",
        "        model = model.to(self.dev)\n",
        "        torch.no_grad()\n",
        "        m = copy.deepcopy(model)\n",
        "        m.eval()\n",
        "        \n",
        "        example_inputs = torch.unsqueeze(calibdata.input_images[10], dim=0)\n",
        "        \n",
        "        qconfig = torch.quantization.get_default_qat_qconfig(\"qnnpack\", 1)\n",
        "        qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
        "\n",
        "        model_prepared = quantize_fx.prepare_qat_fx(m, qconfig_mapping, example_inputs)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            for i in range(10):\n",
        "                x = torch.unsqueeze(calibdata.input_images[i], dim=0).to(self.dev)\n",
        "                model_prepared(x)\n",
        "                    \n",
        "        trained_model = self.train(model_prepared)\n",
        "        trained_model = trained_model.to(torch.device('cpu'))\n",
        "        model_quantized = quantize_fx.convert_fx(trained_model)\n",
        "        model_quantized.train(False)\n",
        "        model_quantized.eval()\n",
        "        return model_quantized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6ANKjcENpKe3"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, name, testset, dev, batch_size):\n",
        "    test = ImageNetTest(batch_size)\n",
        "    print(name, 'FP32')\n",
        "    model = model.to(dev)\n",
        "    fp32 = test.test(model, testset, dev)\n",
        "    dev = torch.device(\"cpu\")\n",
        "    model = model.to(dev)\n",
        "    model_int8 = quantize_model(model, testset, dev)\n",
        "    print(name, 'INT8')\n",
        "    int8 =  test.test(model_int8, testset, dev)\n",
        "    ptq_ = PTQ(model, testset, dev)\n",
        "    bias_corr = ptq_.quantize_model()\n",
        "    print(name, 'PTQ INT8')\n",
        "    ptq_int8 = test.test(bias_corr, testset, dev)\n",
        "    \n",
        "    return fp32, int8, ptq_int8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GetSegmentationDataset(Dataset):\n",
        "    def __init__(self, input_path, target_path, bool_augmentation):\n",
        "        super(GetSegmentationDataset, self).__init__()\n",
        "        self.preprocess_input = transforms.Compose([\n",
        "                      transforms.Resize(256),\n",
        "                      transforms.ToTensor(),\n",
        "            ])\n",
        "\n",
        "        self.preprocess_target = transforms.Compose([\n",
        "                            transforms.Resize(256, interpolation=transforms.InterpolationMode.NEAREST),\n",
        "                            transforms.ToTensor(),\n",
        "                    ])\n",
        "\n",
        "        self.augmentation = transforms.Compose([\n",
        "                transforms.RandomRotation(30, interpolation=Image.NEAREST), \n",
        "                transforms.Resize(256, interpolation=Image.NEAREST),\n",
        "                transforms.ToTensor()])\n",
        "        self.input_images = self.load_images(input_path, bool_augmentation, self.preprocess_input)\n",
        "        self.target_images = self.load_images(target_path, bool_augmentation, self.preprocess_target)\n",
        "        print(self.target_images[0].shape)\n",
        "\n",
        "    def load_images(self, path, bool_augmentation, preprocess_func):\n",
        "        images = os.listdir(path)\n",
        "        image_names = list(filter(lambda x: x.endswith(\".png\"), images))\n",
        "        sorted_images = natsort.natsorted(image_names)\n",
        "        loaded_images = []\n",
        "\n",
        "        for count, image_name in enumerate(sorted_images):\n",
        "            image = Image.open(os.path.join(path, image_name)).convert('L')\n",
        "            image_tensor = preprocess_func(image).float()\n",
        "            loaded_images.append(image_tensor)\n",
        "\n",
        "            if bool_augmentation and count % 3 == 0:\n",
        "                image_tensor = self.augmentation(image).float()\n",
        "                loaded_images.append(image_tensor)\n",
        "\n",
        "        return loaded_images\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.input_images[index], self.target_images[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchmetrics\n",
        "\n",
        "class SegmentationTest():\n",
        "    def __init__(self, dev, batch_size):\n",
        "        super(SegmentationTest, self).__init__()\n",
        "        self.dev = dev\n",
        "        self.batch_size = batch_size\n",
        "        self.Dice = torchmetrics.Dice(zero_division=1.0, threshold=0.5).to(self.dev)\n",
        "        \n",
        "    def test(self, net, test_loader, name):\n",
        "        test_dice_losses = []\n",
        "        net.train(False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data = data.to(self.dev)\n",
        "                target = target.to(self.dev)\n",
        "                output = net(data)                \n",
        "                dice_loss_value = self.Dice(output, target.int())\n",
        "                test_dice_losses.append(dice_loss_value.item())\n",
        "                \n",
        "        print('Test dice loss: ', np.average(np.asarray(test_dice_losses)))\n",
        "        \n",
        "        with open('./segmentation_results/' + name + '/test_results.txt', 'w') as f:\n",
        "            f.write('Test dice loss: ' + str(np.average(np.asarray(test_dice_losses))) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Image Classification\"\"\"\n",
        "testset = GetDataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\orsolya.bankovi/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EfficientNet_b0 FP32\n",
            "Accuracy:  0.743\n",
            "EfficientNet_b0 INT8\n",
            "Accuracy:  0.602\n",
            "EfficientNet_b0 PTQ INT8\n",
            "Accuracy:  0.633\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\orsolya.bankovi/.cache\\torch\\hub\\pytorch_vision_main\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MaxViT FP32\n",
            "Accuracy:  0.818\n",
            "MaxViT INT8\n",
            "Accuracy:  0.049\n",
            "MaxViT PTQ INT8\n",
            "Accuracy:  0.102\n"
          ]
        }
      ],
      "source": [
        "with open('./imagenet_accuracy.txt', 'w') as f:\n",
        "        efficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\n",
        "        fp32, int8, ptq_int8 = evaluate_model(efficientnet, 'EfficientNet_b0', testset, dev, 1)\n",
        "        f.write('EfficientNet FP32: ' + str(fp32) + '\\n')\n",
        "        f.write('EfficientNet INT8: ' + str(int8) + '\\n')\n",
        "        f.write('EfficientNet PTQ INT8: ' + str(ptq_int8) + '\\n')\n",
        "\n",
        "        maxvit_t = torch.hub.load('pytorch/vision', 'maxvit_t', pretrained=True)\n",
        "        fp32, int8, ptq_int8 = evaluate_model(maxvit_t, 'MaxViT', testset, dev, 1)\n",
        "        f.write('MaxViT FP32: ' + str(fp32) + '\\n')\n",
        "        f.write('MaxViT INT8: ' + str(int8) + '\\n')\n",
        "        f.write('MaxViT PTQ INT8: ' + str(ptq_int8) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256, 256])\n",
            "torch.Size([1, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "train_set=GetSegmentationDataset(input_path='./unet_/train/original', \n",
        "                            target_path='./unet_/train/inverse', \n",
        "                            bool_augmentation=True)\n",
        "test_set = GetSegmentationDataset(input_path='./unet_/test/original', \n",
        "                        target_path='./unet_/test/inverse', \n",
        "                        bool_augmentation=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, 1, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UNet FP32\n",
            "Test dice loss:  0.9023179816525607\n",
            "UNet INT8\n",
            "Test dice loss:  0.8349785774691207\n",
            "UNet PTQ INT8\n",
            "Test dice loss:  0.8363506468722508\n",
            "best epoch:  9\n",
            "best dice:  0.6170561487072861\n",
            "UNet QAT INT8\n",
            "Test dice loss:  0.8705367499518962\n"
          ]
        }
      ],
      "source": [
        "import unet\n",
        "\n",
        "UNet = torch.load('./unet_/finished_trained_net.pt' )\n",
        "\n",
        "dev = torch.device(\"cpu\")\n",
        "UNet = UNet.to(dev)\n",
        "Test = SegmentationTest(dev=dev, batch_size=1)\n",
        "print('UNet FP32')\n",
        "Test.test(UNet, test_loader, 'original')\n",
        "unet_int8 = quantize_model(UNet, test_set, dev, backend=\"qnnpack\")\n",
        "print('UNet INT8')\n",
        "Test.test(unet_int8, test_loader, 'INT8')\n",
        "ptq_unet = PTQ(UNet, test_set, dev)\n",
        "bias_corr = ptq_unet.quantize_model(backend=\"qnnpack\")\n",
        "print('UNet PTQ INT8')\n",
        "Test.test(bias_corr, test_loader, 'int8_ptq')\n",
        "qat = QuantizationAwareTraining(train_set, test_set, batch_size=8)\n",
        "qat_net = qat.quantize_model(UNet, train_set)\n",
        "print('UNet QAT INT8')\n",
        "Test.test(qat_net, test_loader, 'int8_qat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\orsolya.bankovi/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AlexNet FP32\n",
            "Accuracy:  0.539\n",
            "AlexNet INT8\n",
            "Accuracy:  0.528\n",
            "AlexNet PTQ INT8\n",
            "Accuracy:  0.528\n",
            "ShuffleNet FP32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\orsolya.bankovi/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.669\n",
            "ShuffleNet INT8\n",
            "Accuracy:  0.653\n",
            "ShuffleNet PTQ INT8\n",
            "Accuracy:  0.653\n"
          ]
        }
      ],
      "source": [
        "alexnet = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
        "fp32, int8, ptq_int8 = evaluate_model(alexnet, 'AlexNet', testset, dev, 1)\n",
        "\n",
        "shufflenet = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x1_0', pretrained=True)\n",
        "fp32, int8, ptq_int8 = evaluate_model(shufflenet, 'ShuffleNet', testset, dev, 1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
